---
layout: post
title: "Kubernetes Basics: How the Control Plane Really Works"
date: 2025-07-01 10:00:00 +0000
categories: [kubernetes]
tags: [kubernetes, devops, control-plane, kube-apiserver]
image: /assets/images/kubernetes.jpg
permalink: /kubernetes/basics
---

Most Kubernetes issues are not caused by YAML syntax or missing fields.They come from misunderstanding when Kubernetes acts and who is responsible for what.

I have seen many situations where:
- `kubectl apply` succeeds
- Pods exist
- And the application still doesn’t work

This post explains why that happens, using the same commands and behavior I have seen repeatedly.

Kubernetes Is a Control System, Not a Deployment Tool
When you run:
```bash
kubectl apply -f deployment.yaml
```
Kubernetes does not:
- Create a pod immediately
- Start your application
- Verify your app is healthy

It only does one thing: Stores your desired state

Everything else happens asynchronously.
This distinction is the root of many misunderstandings.

## The Control Plane Components (At a Practical Level)

You don’t need to memorize internals  but you do need to know who is responsible for what.

kube-apiserver:
- Entry point for everything
- Validates and stores objects
- Does not create pods itself
If `kubectl apply` succeeds, it only means the API server accepted the request.

etcd:
- Key-value store
- Holds the cluster’s desired and current state
If the state exists in etcd, Kubernetes believes it should exist even if nothing is running.

Controller Manager:
- Watches desired state vs actual state
- Decides what should happen next
- Creates ReplicaSets, Pods, endpoints, etc.
Controllers reconcile continuously not instantly.

Scheduler:
- Assigns pods to nodes
- Only works on Pending pods
- Refuses to schedule if constraints are not met
If a pod is Pending, the scheduler has not found a suitable node.

## Worker Node (Where Work Actually Happens)

kubelet:
- Entry point for Pods on the node
- Ensures containers are running as defined
- Talks only to the API server
- Does not schedule Pods only runs assigned ones
If a Pod exists but isn’t running, kubelet is usually involved.

Kube-Proxy:
- Handles Service networking
- Programs iptables/IPVS rules for traffic routing
- Routes Service IP traffic to Pod IPs
- Does not load-balance at L7
If Service exists but traffic fails, kube-proxy is a suspect.

Container Runtime:
- Runs the actual containers
- Pulls images and starts/stops containers
- Kubernetes never runs containers directly
If the runtime is broken, Pods will never start.

Desired State vs Actual State (This Is Critical)

Kubernetes always tries to answer one question: “What should exist?”
It does not ask:
- Is your app reachable?
- Is your config correct?
- Does your dependency exist?

Example:
```yaml
replicas: 3
```
Kubernetes only cares that:
- 3 pods should exist
- Not whether they work

This is why you often see:
- Pods running but app broken
- Green deployments with failing services

```bash
kubectl apply -f deployment.yaml
```
Result
```bash
deployment.apps/myapp created
```
“kubectl apply succeeded” Does Not Mean “App Is Ready”

This is one of the most common assumptions.
Success only means:
- YAML was valid
- API server accepted it
It does not mean:
- Image was pulled
- Container started
- Service is reachable

Those failures happen after apply.

The Deployment YAML:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        image: myorg/myapp:1.0
        ports:
        - containerPort: 8080
```
Kubernetes accepted this meaning only one thing: the API server stored the desired state. Nothing more.

What Kubernetes Did Next (Quietly)
Running
```bash
kubectl get deploy myapp
```
Showed
```bash
READY   UP-TO-DATE   AVAILABLE
0/2     2            0
```
This was the first hint:
- Desired state exists
- Actual state does not
- Kubernetes was still reconciling.

Watching the Control Plane at Work
Checking ReplicaSets:
```bash
kubectl get rs
NAME                DESIRED   CURRENT   READY
myapp-5d8f7d6f9c    2         0         0
```
Then Pods:
```bash
kubectl get pods
myapp-5d8f7d6f9c-abcde   Pending
myapp-5d8f7d6f9c-fghij   Pending
```

This sequence made it clear:
- Deployment created a ReplicaSet
- ReplicaSet requested Pods
- Scheduler had not placed them yet

Why This Matters
At no point did Kubernetes promise:
- Pods would start immediately
- Containers would run
- Application would be reachable
Each component was doing only its responsibility.

Where Most Kubernetes Issues Actually Live

From experience, most issues fall into these layers:
> Scheduling layer (Pending pods, resource constraints)
> Runtime layer (CrashLoopBackOff, ImagePullBackOff)
> Networking layer (Service exists, but traffic doesn’t flow)
> Configuration layer (Secrets, ConfigMaps, env vars)

Kubernetes Is Always Reconciling
Even after everything is running, Kubernetes keeps checking:
```bash
kubectl delete pod myapp-5d8f7d6f9c-abcde
```
Immediately
```bash
kubectl get pods
```
A new pod appears. This is not Kubernetes “restarting”, it’s Kubernetes enforcing the desired state.

Desired State vs Actual State (Seen in Practice)

Kubernetes only tracks:
- What should exist
- Not whether it works

If you ask for 2 replicas, Kubernetes ensures:
- 2 Pods exist

It does not care if:
- App returns 500s
- Dependencies are missing
- Traffic can’t reach it

That responsibility is outside the control plane.

How This Explains Most Kubernetes Issues

Once this clicked for me, many issues made sense:
- Pods restarting endlessly
- Services existing but not routing
- Successful deploys with broken apps
Kubernetes wasn’t failing, it was doing exactly what it was told.

The next posts in this series will build on this foundation and focus on the issues you actually face when running workloads on Kubernetes.
