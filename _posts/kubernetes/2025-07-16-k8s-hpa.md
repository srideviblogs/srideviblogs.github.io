---
layout: post
title: "Traffic Spiked, Pods Didn’t: A Silent Kubernetes HPA Failure"
date: 2025-07-16 10:00:00 +0000
categories: kubernetes
tags: [kubernetes, hpa, autoscaling, debug, metrics]
image: /assets/images/kubernetes.jpg
permalink: /kubernetes/traffic-spiked-pods-didnt-hpa-failure/
---

This happened during a routine production deployment. A new feature was released. Traffic increased gradually. Application latency started rising.
But the number of pods never changed.

```bash
kubectl get pods
NAME                           READY   STATUS    RESTARTS
billing-api-6f8d9f7b9c-abc12   1/1     Running   0
billing-api-6f8d9f7b9c-def34   1/1     Running   0
```
No crashes. No errors. Just slower responses.

Why This Was Confusing
An HPA already existed.
```bash
kubectl get hpa
NAME           REFERENCE                     TARGETS         MINPODS   MAXPODS
billing-api   Deployment/billing-api        15%/70%         2         10
```
From the outside, everything looked correct:
- HPA was configured
- Metrics server was healthy

The Wrong Assumption
The assumption was simple: 
- More traffic means higher CPU.
But Kubernetes doesn’t scale on traffic. It scales on metrics.

So we checked actual CPU usage.

```bash
kubectl top pods
NAME                           CPU(cores)   MEMORY(bytes)
billing-api-6f8d9f7b9c-abc12   35m           220Mi
billing-api-6f8d9f7b9c-def34   40m           230Mi
```
CPU usage was low.

What Actually Changed
- The application behavior had evolved.
- Earlier versions were CPU-heavy.
- The new version wasn’t.

Most of the work had moved to:
- External APIs
- Databases
- Network calls

From Kubernetes’ point of view:
- CPU usage was healthy
- Pods were within limits
- No scaling was required

The Silent Failure
This was the key realization:
- HPA was working exactly as configured, but the metric no longer represented load.

There was no warning because:
- HPA logic was correct
- Metrics were available
- Nothing was technically broken

The system did exactly what it was told to do.

The fix was small but important.

1. Adjusting Resource Requests
CPU requests were too high, making real usage look insignificant.

Before:
```yaml
resources:
  requests:
    cpu: "500m"
```
After
```yaml
resources:
  requests:
    cpu: "100m"
```
This made actual CPU usage visible to the HPA.

Aligning HPA With Real Load

The HPA configuration itself didn’t need a redesign, just realistic thresholds.

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
spec:
  scaleTargetRef:
    kind: Deployment
    name: billing-api
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```
Applied the change:
```bash
kubectl apply -f hpa.yaml
```
Within minutes, new pods were created as traffic increased.

Why This Happens in Real Clusters

This issue shows up when:
- Application behavior changes
- CI deploys new code, but autoscaling logic stays the same
- Teams assume CPU always represents load
- HPA configs are set once and forgotten

Kubernetes didn’t fail. The assumptions around it did.

How I Think About HPA Now

When HPA stays silent, I check:

- hat metric is it scaling on?
- Does that metric still represent real load?
- Are resource requests realistic?
- Did the application change recently?
If the metric is quiet, HPA will be quiet too.

## Final Thought

Silent failures are the hardest ones.
> No alerts.
> No crashes.
> No obvious errors.

Just a system behaving exactly as configured, even when the workload has moved on.
