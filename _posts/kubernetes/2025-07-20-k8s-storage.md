---
layout: post
title: "Pods Stuck in Pending: The ReadWriteOnce Storage Trap"
date: 2025-07-20 10:00:00 +0000
categories: kubernetes
tags: [kubernetes, pvc, pv, storage]
image: /assets/images/kubernetes.jpg
permalink: /kubernetes/pods-stuck-pending-readwriteonce-trap/
---

This showed up during a routine rollout. The Deployment was updated. Old pods were terminated one by one. New pods were created.
But they never moved past `Pending`.

```bash
kubectl get pods
NAME                           READY   STATUS    RESTARTS
reporting-api-7c9d8f6b5d-abc12  0/1     Pending   0
```
The previous pod was still running. The new pod never started.

At a glance:
- Nodes were healthy
- PVC existed and was bound
- Storage looked available
This didn’t look like a storage failure. But the pod wouldn’t schedule.

The Clue Was in the Events
Describing the pod revealed the real issue.
```bash
kubectl describe pod reporting-api-7c9d8f6b5d-abc12
```
Near the bottom:
```text
Multi-Attach error for volume "pvc-4a1b2c3d":
Volume is already used by pod reporting-api-7c9d8f6b5d-xyz89
```
That message pointed directly at storage semantics.

What Was Actually Wrong
The PVC was created with ReadWriteOnce.
```bash
accessModes:
  - ReadWriteOnce
```
This means:
- The volume can be mounted by one node at a time
- Not one pod, one node

During the rollout:
- Old pod was still attached
- New pod tried to mount the same PVC
- Kubernetes blocked the second attachment
So the pod stayed Pending.

This is the core misunderstanding:
ReadWriteOnce does not mean one pod. It means one node. If two pods land on different nodes, the second one will wait forever.

The Fix
We had two realistic options.

Option 1: Use RollingUpdate With maxUnavailable: 1
Ensured the old pod detached before the new one started.
```yaml
strategy:
  rollingUpdate:
    maxUnavailable: 1
    maxSurge: 0
```
This prevented concurrent mounts.

Option 2: Use Storage That Supports ReadWriteMany
If parallel pods are required:
```yaml
accessModes:
  - ReadWriteMany
```
Using NFS / EFS / Azure Files instead of block storage.

This issue shows up when:
- Deployments use PVCs with RWO
- Rolling updates overlap pods
- Nodes are replaced or drained
- Teams assume volumes are pod-scoped
Everything is correct, just incompatible.

When pods are Pending and PVCs look fine, now I check:
- Pod events for Multi-Attach
- PVC access mode
- Number of replicas
- RollingUpdate strategy
- Node placement
Storage failures are usually design mismatches, not outages.

## Final Thought

Kubernetes storage is strict by design. If the access mode doesn’t match the workload, pods will wait.
