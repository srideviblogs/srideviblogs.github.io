---
layout: post
title: "Kubernetes RBAC: Why Access Is Denied Even When It Looks Correct"
date: 2025-07-25 10:00:00 +0000
categories: kubernetes
tags: [kubernetes, rbac, access-control]
image: /assets/images/kubernetes.jpg
permalink: /kubernetes/kubernetes-rbac-access-denied/
---

This happened during a deployment automation test. A developer tried to create a namespace-scoped resource:
```bash
kubectl apply -f app-config.yaml -n dev-team
```
But got this error:
```text
Error from server (Forbidden): namespaces "dev-team" is forbidden: User "alice" cannot create resource "configmaps" in API group "" in the namespace "dev-team"
```
The user checked roles and bindings:
```bash
kubectl get rolebinding -n dev-team
NAME            ROLE              USERS
dev-team-admin  Role/dev-team-admin  alice
```
Role:
```yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: dev-team-admin
rules:
- apiGroups: [""]
  resources: ["configmaps"]
  verbs: ["create", "get", "list", "update", "delete"]
```
From the manifest, it looked like alice should have access. Yet Kubernetes denied the request.

RBAC roles are namespace-scoped unless they are ClusterRole.
The developer ran:
```bash
kubectl apply -f app-config.yaml -n dev-team
```
But `kubectl config get-contexts` showed the current context namespace was `default`:
```bash
CURRENT   NAME           NAMESPACE
*         dev-cluster    default
```
Kubernetes evaluates access based on the current context namespace, not the namespace in the manifest alone.

What Was Actually Wrong
The user was authenticated correctly, but the request was evaluated against a namespace where no RoleBinding existed.
```bash
kubectl auth can-i create configmaps -n default
no
```
Kubernetes denied access even though the role in `dev-team` was correct.

The key lesson:
RBAC checks the namespace of the request, not just the resource manifest.
ClusterRoleBindings are global. RoleBindings are namespace-scoped. Context matters.
Even correct RoleBindings wonâ€™t work if the current context namespace is different.

The Fix
Option 1: Switch Namespace Context
```bash
kubectl config set-context --current --namespace=dev-team
kubectl apply -f app-config.yaml
```
Option 2: Use a ClusterRole / ClusterRoleBinding
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: dev-team-admin-global
subjects:
- kind: User
  name: alice
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: dev-team-admin
  apiGroup: rbac.authorization.k8s.io
```
This makes the role effective across all namespaces.

This is extremely common when:
- Developers switch clusters frequently
- CI pipelines create resources in multiple namespaces
- RoleBindings exist but context is incorrect
- ClusterRoleBindings are not used when global access is required

When access seems denied:
- Check `kubectl auth can-i <verb> <resource> -n <namespace>`
- Confirm current context namespace (`kubectl config view --minify | grep namespace`)
- Check Role vs ClusterRole and RoleBinding vs ClusterRoleBinding
- Review any namespace-specific restrictions
Kubernetes always enforces RBAC rules correctly, you just need to ask in the right namespace.

RBAC is strict by design. Even a minor context mismatch or namespace oversight can block access.
